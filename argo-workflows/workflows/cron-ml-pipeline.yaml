apiVersion: argoproj.io/v1alpha1
kind: CronWorkflow
metadata:
  name: ml-pipeline-cron
  namespace: argo
  labels:
    monitoring: "enabled"
    pipeline-type: "ml-cv"
    schedule-type: "cron"
spec:
  schedule: "*/10 * * * *" # Every 10 minutes for testing
  concurrencyPolicy: "Forbid"
  startingDeadlineSeconds: 300

  workflowSpec:
    entrypoint: cron-ml-pipeline
    arguments:
      parameters:
      - name: dataset-name
        value: "cv-dataset-cron-{{workflow.creationTimestamp}}"
      - name: batch-size
        value: "1000"

    templates:
    - name: cron-ml-pipeline
      dag:
        tasks:
        # Phase 1: Data Pipeline Start
        - name: pipeline-start
          template: timing-marker
          arguments:
            parameters:
            - name: event-type
              value: "cron_pipeline_start"
            - name: task-name
              value: "ml-cron-pipeline"

        # Phase 2: Data Validation & Preparation
        - name: validate-data-source
          template: data-validation-task
          dependencies: [ pipeline-start ]
          arguments:
            parameters:
            - name: dataset
              value: "{{workflow.parameters.dataset-name}}"

        - name: check-data-freshness
          template: freshness-check-task
          dependencies: [ pipeline-start ]

        # Phase 3: Data Processing (parallel)
        - name: extract-features
          template: feature-extraction-task
          dependencies: [ validate-data-source, check-data-freshness ]
          arguments:
            parameters:
            - name: dataset
              value: "{{workflow.parameters.dataset-name}}"
            - name: batch-size
              value: "{{workflow.parameters.batch-size}}"

        - name: augment-data
          template: data-augmentation-task
          dependencies: [ validate-data-source, check-data-freshness ]
          arguments:
            parameters:
            - name: dataset
              value: "{{workflow.parameters.dataset-name}}"

        # Phase 4: Model Training & Evaluation
        - name: train-model
          template: model-training-task
          dependencies: [ extract-features, augment-data ]
          arguments:
            parameters:
            - name: dataset
              value: "{{workflow.parameters.dataset-name}}"

        - name: evaluate-model
          template: model-evaluation-task
          dependencies: [ train-model ]
          arguments:
            parameters:
            - name: dataset
              value: "{{workflow.parameters.dataset-name}}"

        # Phase 5: Results & Cleanup
        - name: save-results
          template: results-storage-task
          dependencies: [ evaluate-model ]
          arguments:
            parameters:
            - name: dataset
              value: "{{workflow.parameters.dataset-name}}"

        - name: pipeline-end
          template: timing-marker
          dependencies: [ save-results ]
          arguments:
            parameters:
            - name: event-type
              value: "cron_pipeline_complete"
            - name: task-name
              value: "ml-cron-pipeline"

    # Task Templates with Timing
    - name: timing-marker
      inputs:
        parameters:
        - name: event-type
        - name: task-name
      container:
        image: python:3.9-slim
        command: [ python, -c ]
        args:
        - |
          import json
          from datetime import datetime, timezone

          event_type = "{{inputs.parameters.event-type}}"
          task_name = "{{inputs.parameters.task-name}}"
          timestamp = datetime.now(timezone.utc).isoformat()

          timing_record = {
              "@timestamp": timestamp,
              "workflow": {
                  "name": "{{workflow.name}}",
                  "uid": "{{workflow.uid}}",
                  "cron_schedule": "*/10 * * * *",
                  "dataset": "{{workflow.parameters.dataset-name}}"
              },
              "event": {
                  "type": event_type,
                  "task": task_name
              },
              "cron": {
                  "execution_time": timestamp,
                  "expected_duration_minutes": 8
              }
          }

          print("üìÖ CRON TIMING RECORD:")
          print(json.dumps(timing_record, indent=2))

    - name: data-validation-task
      inputs:
        parameters:
        - name: dataset
      container:
        image: python:3.9-slim
        command: [ python, -c ]
        args:
        - |
          import time
          import random

          dataset = "{{inputs.parameters.dataset}}"
          print(f"üîç Validating data source: {dataset}")

          # Simulate data validation work
          validation_time = random.uniform(10, 30)  # 10-30 seconds
          time.sleep(validation_time)

          # Random validation results
          records_found = random.randint(800, 1200)
          validation_score = random.uniform(0.85, 0.98)

          print(f"‚úÖ Data validation complete:")
          print(f"  Records found: {records_found}")
          print(f"  Validation score: {validation_score:.3f}")
          print(f"  Duration: {validation_time:.1f}s")

    - name: freshness-check-task
      container:
        image: python:3.9-slim
        command: [ python, -c ]
        args:
        - |
          import time
          import random
          from datetime import datetime, timedelta

          print("üìÖ Checking data freshness...")

          check_time = random.uniform(5, 15)
          time.sleep(check_time)

          # Simulate freshness check
          last_update = datetime.now() - timedelta(hours=random.randint(1, 6))
          freshness_hours = (datetime.now() - last_update).total_seconds() / 3600

          print(f"‚úÖ Data freshness check complete:")
          print(f"  Last update: {last_update.strftime('%Y-%m-%d %H:%M')}")
          print(f"  Data age: {freshness_hours:.1f} hours")
          print(f"  Check duration: {check_time:.1f}s")

    - name: feature-extraction-task
      inputs:
        parameters:
        - name: dataset
        - name: batch-size
      container:
        image: python:3.9-slim
        command: [ python, -c ]
        args:
        - |
          import time
          import random

          dataset = "{{inputs.parameters.dataset}}"
          batch_size = int("{{inputs.parameters.batch-size}}")

          print(f"üîß Extracting features from: {dataset}")
          print(f"Batch size: {batch_size}")

          # Simulate feature extraction with batches
          batches = 5
          total_features = 0
          total_time = 0

          for batch in range(1, batches + 1):
              batch_time = random.uniform(15, 25)  # 15-25 seconds per batch
              features_extracted = random.randint(150, 250)
              
              print(f"  Batch {batch}/{batches}: {batch_time:.1f}s, {features_extracted} features")
              time.sleep(batch_time)
              
              total_features += features_extracted
              total_time += batch_time

          print(f"‚úÖ Feature extraction complete:")
          print(f"  Total features: {total_features}")
          print(f"  Total duration: {total_time:.1f}s")

    - name: data-augmentation-task
      inputs:
        parameters:
        - name: dataset
      container:
        image: python:3.9-slim
        command: [ python, -c ]
        args:
        - |
          import time
          import random

          dataset = "{{inputs.parameters.dataset}}"
          print(f"üé® Augmenting data: {dataset}")

          # Simulate data augmentation techniques
          techniques = ["rotation", "scaling", "color_adjust", "noise_addition", "cropping"]
          augmented_samples = 0
          total_time = 0

          for technique in techniques:
              technique_time = random.uniform(12, 20)
              samples = random.randint(100, 200)
              
              print(f"  {technique}: {technique_time:.1f}s, {samples} samples")
              time.sleep(technique_time)
              
              augmented_samples += samples
              total_time += technique_time

          print(f"‚úÖ Data augmentation complete:")
          print(f"  Total augmented samples: {augmented_samples}")
          print(f"  Total duration: {total_time:.1f}s")

    - name: model-training-task
      inputs:
        parameters:
        - name: dataset
      container:
        image: python:3.9-slim
        command: [ python, -c ]
        args:
        - |
          import time
          import random

          dataset = "{{inputs.parameters.dataset}}"
          print(f"ü§ñ Training model on: {dataset}")

          # Simulate model training epochs  
          epochs = 3
          total_time = 0
          final_accuracy = 0

          for epoch in range(1, epochs + 1):
              epoch_time = random.uniform(25, 40)  # Longer training time
              accuracy = random.uniform(0.70 + (epoch * 0.05), 0.85 + (epoch * 0.03))
              loss = random.uniform(0.8 - (epoch * 0.15), 0.5 - (epoch * 0.05))
              
              print(f"  Epoch {epoch}/{epochs}: {epoch_time:.1f}s")
              print(f"    Accuracy: {accuracy:.3f}, Loss: {loss:.3f}")
              
              time.sleep(epoch_time)
              total_time += epoch_time
              final_accuracy = accuracy

          print(f"‚úÖ Model training complete:")
          print(f"  Final accuracy: {final_accuracy:.3f}")
          print(f"  Total training time: {total_time:.1f}s")

    - name: model-evaluation-task
      inputs:
        parameters:
        - name: dataset
      container:
        image: python:3.9-slim
        command: [ python, -c ]
        args:
        - |
          import time
          import random

          dataset = "{{inputs.parameters.dataset}}"
          print(f"üìä Evaluating model on: {dataset}")

          # Simulate model evaluation
          eval_time = random.uniform(20, 35)

          # Generate evaluation metrics
          precision = random.uniform(0.78, 0.92)
          recall = random.uniform(0.75, 0.90)
          f1_score = 2 * (precision * recall) / (precision + recall)

          time.sleep(eval_time)

          print(f"‚úÖ Model evaluation complete:")
          print(f"  Precision: {precision:.3f}")
          print(f"  Recall: {recall:.3f}")
          print(f"  F1-Score: {f1_score:.3f}")
          print(f"  Evaluation time: {eval_time:.1f}s")

    - name: results-storage-task
      inputs:
        parameters:
        - name: dataset
      container:
        image: python:3.9-slim
        command: [ python, -c ]
        args:
        - |
          import time
          import random

          dataset = "{{inputs.parameters.dataset}}"
          print(f"üíæ Storing results for: {dataset}")

          # Simulate results storage
          storage_tasks = ["save_model", "save_metrics", "update_registry", "create_report"]
          total_time = 0

          for task in storage_tasks:
              task_time = random.uniform(8, 15)
              print(f"  {task}: {task_time:.1f}s")
              time.sleep(task_time)
              total_time += task_time

          print(f"‚úÖ Results storage complete:")
          print(f"  Total storage time: {total_time:.1f}s")
          print(f"üîó Integration point: Use S3Utils for model artifact storage") 
